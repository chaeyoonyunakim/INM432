{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNa6PF12g/QXgxrx1m0dQPY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chaeyoonyunakim/NLP-IR-QA/blob/main/Untitled3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KJTuC9uKBmm3"
      },
      "source": [
        "## 0. Environment setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6KLY3vxp15Xh",
        "outputId": "781d8ae1-0ddb-4193-f460-7fef68c542b6"
      },
      "source": [
        "from google.colab import drive # mount google drive\n",
        "drive.mount('/content/drive') # authorization\n",
        "%ls -l \"/content/drive/My Drive/2021_INM363_SMART/smarttask_dbpedia_train.json\""
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "-rw------- 1 root root 3601280 Feb 23  2021 '/content/drive/My Drive/2021_INM363_SMART/smarttask_dbpedia_train.json'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MF1IBXJoBdhd",
        "outputId": "7c990805-cf4b-48c7-8304-bf6aa19afbaf"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu Sep  9 14:08:24 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 470.63.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   53C    P8    31W / 149W |      0MiB / 11441MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "THji5Cj_2rMB"
      },
      "source": [
        "# basics\n",
        "import pandas as pd\n",
        "import json\n",
        "import numpy as np\n",
        "from numpy import mean\n",
        "from numpy import std\n",
        "np.random.seed(20211001)\n",
        "import time"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OyG2VFkZ2w94",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd415e59-9caf-461d-bda9-e02d636615f6"
      },
      "source": [
        "# nltk imports\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from collections import defaultdict\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "#set(stopwords.words('english'))\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5zLCcEjVzty3"
      },
      "source": [
        "# Algos\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn import svm\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import GaussianNB"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "re7UB28r2z-P"
      },
      "source": [
        "# scikit-learn Tools for modelling\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_recall_fscore_support, confusion_matrix, classification_report, accuracy_score"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tnc8JCMFT1pI"
      },
      "source": [
        "# torch imports\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import random_split\n",
        "from torchvision.utils import make_grid"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zBdXkiJiTxdS",
        "outputId": "ab677619-3e92-4221-83d6-eab0df33bed0"
      },
      "source": [
        "# Check CPU / GPU environment\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda:0\") # Training on GPU\n",
        "    print(\"GPU is available\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\") # Training on CPU\n",
        "    print(\"GPU not available, CPU used\")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU is available\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VwTe9wMf3MpC"
      },
      "source": [
        "## 1. Data Loading & Manipulation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3jIXs6yPCS1V"
      },
      "source": [
        "***Tabular representation of the training dataset***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        },
        "id": "N1GxpONV3AD5",
        "outputId": "480cc6cf-bcfc-451f-cec8-2b7c9a034940"
      },
      "source": [
        "df_train = pd.read_json('/content/drive/My Drive/2021_INM363_SMART/smarttask_dbpedia_train.json')\n",
        "df_test = pd.read_json('/content/drive/My Drive/2021_INM363_SMART/smarttask_dbpedia_test.json')\n",
        "df_train.head(3)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>question</th>\n",
              "      <th>category</th>\n",
              "      <th>type</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>dbpedia_1177</td>\n",
              "      <td>Was Jacqueline Kennedy Onassis a follower of M...</td>\n",
              "      <td>boolean</td>\n",
              "      <td>[boolean]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>dbpedia_14427</td>\n",
              "      <td>What is the name of the opera based on Twelfth...</td>\n",
              "      <td>resource</td>\n",
              "      <td>[dbo:Opera, dbo:MusicalWork, dbo:Work]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>dbpedia_16615</td>\n",
              "      <td>When did Lena Horne receive the Grammy Award f...</td>\n",
              "      <td>literal</td>\n",
              "      <td>[date]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              id  ...                                    type\n",
              "0   dbpedia_1177  ...                               [boolean]\n",
              "1  dbpedia_14427  ...  [dbo:Opera, dbo:MusicalWork, dbo:Work]\n",
              "2  dbpedia_16615  ...                                  [date]\n",
              "\n",
              "[3 rows x 4 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QgeZu4bD3JXL",
        "outputId": "259e55d9-71f3-47a5-c41f-06a50dc9fc07"
      },
      "source": [
        "df_train.shape, df_test.shape"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((17571, 4), (4381, 4))"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LJCtM3Ip3MEq",
        "outputId": "1c88ce6e-ff92-4a01-8bad-b4a3706c874e"
      },
      "source": [
        "df_test.isnull().sum()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "id          0\n",
              "question    0\n",
              "category    0\n",
              "type        0\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V2MxRjfm3St2",
        "outputId": "d9404d97-5440-4858-e60d-fe27ba65793c"
      },
      "source": [
        "df_train.dropna(subset=['id', 'question', 'category'], inplace=True)\n",
        "df_train.shape"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(17528, 4)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hFPtKOu0hU8R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22fb27ad-0bed-48df-a52f-62b0838d1d50"
      },
      "source": [
        "text_train = df_train.question.values\n",
        "text_test = df_test.question.values\n",
        "text_test"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['How many ingredients are in the grain} ?',\n",
              "       'Is the case fatality rate of Fournier gangrene fewer than 9.0?',\n",
              "       'Does the shelf life of spinach equal 8?', ...,\n",
              "       'What is the location of Edmonton',\n",
              "       'In which department does Raymond Baldwin work?',\n",
              "       'What is Actorenregister ID for Utrecht University?'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RD4T9qTH3din"
      },
      "source": [
        "## 1. Preprocessing the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fHAR7GTHENZs"
      },
      "source": [
        "***Clean the corpus***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S-YLMZeAhBkl"
      },
      "source": [
        "# training dataset\n",
        "vocab_train = []\n",
        "tokens_train = []\n",
        "\n",
        "for sent in text_train[:100]: # take sample 9000 to run a small batch\n",
        "    x = word_tokenize(sent) # tokenization (strip sentences by word)\n",
        "    sentence = [w.lower() for w in x if w.isalpha()] # lower alphabets (filtering non-string characters and then decapitalization)\n",
        " \n",
        "    for word in sentence:\n",
        "        if word not in vocab_train:\n",
        "            vocab_train.append(word) # remove duplicates\n",
        "            \n",
        "for word in vocab_train:\n",
        "    if word not in stopwords.words(): # filter stopwords out\n",
        "        tokens_train.append(word)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4xiQauWwy3uw"
      },
      "source": [
        "# test dataset\n",
        "vocab_test = []\n",
        "tokens_test = []\n",
        "\n",
        "for sent in text_test:\n",
        "    x = word_tokenize(sent)\n",
        "    sentence = [w.lower() for w in x if w.isalpha()]\n",
        " \n",
        "    for word in sentence:\n",
        "        if word not in vocab_test:\n",
        "            vocab_test.append(word)\n",
        "            \n",
        "for word in vocab_test:\n",
        "    if word in tokens_train:\n",
        "        if word not in stopwords.words():\n",
        "            tokens_test.append(word)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "04BOKJ9wFfWp"
      },
      "source": [
        "***Text normalization***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NG3vGIJ1gOdO"
      },
      "source": [
        "wordnet_lemmatizer = WordNetLemmatizer()\n",
        "ps = PorterStemmer()\n",
        "\n",
        "def normalize_word(w):\n",
        "   word1 = wordnet_lemmatizer.lemmatize(w, pos = \"n\")\n",
        "   word2 = wordnet_lemmatizer.lemmatize(word1, pos = \"v\")\n",
        "   word3 = wordnet_lemmatizer.lemmatize(word2, pos = (\"a\"))\n",
        "   word = ps.stem(word3)\n",
        "   return word\n",
        "\n",
        "\n",
        "## Define the Bag of Words model function\n",
        "def create_bow(word_list):\n",
        "  ind = 0 \n",
        "  bow = {}\n",
        "  for w in word_list:\n",
        "    _w = normalize_word(w)\n",
        "    if _w not in bow:\n",
        "      bow[_w] = ind \n",
        "      ind += 1 \n",
        "  return bow\n",
        "\n",
        "bow_train = create_bow(tokens_train)\n",
        "bow_test = create_bow(tokens_test)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TnMel3J5GQ8M"
      },
      "source": [
        "***Vectorization***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rmspa3QgGPY9"
      },
      "source": [
        "## Assign an index to the word\n",
        "label_map = {\"boolean\": 1, \"literal\":2, \"resource\":3}\n",
        "\n",
        "def map_to_vec(df, bow):\n",
        "  # add 1 for now just for the category\n",
        "  # requires additional cols for literals and sub resources for later\n",
        "  ncols = len(bow) + 1 \n",
        "  data = np.zeros(shape = (df.shape[0], ncols))\n",
        "  \n",
        "  for i in range(df.shape[0]):\n",
        "    # set the label\n",
        "    data[i, -1] = label_map[df.iloc[i, 2]]\n",
        "    # parse the sentence\n",
        "    que = df.iloc[i, 1]\n",
        "    for w in word_tokenize(que):\n",
        "      w = w.lower()\n",
        "      if w.isalpha():\n",
        "        # normalize word\n",
        "        w_norm = normalize_word(w)\n",
        "        if w_norm in bow:\n",
        "          # print(f\"({i}, {w_norm})\")\n",
        "          data[i, bow[w_norm]] += 1 \n",
        "  return data\n",
        "\n",
        "vec_train = map_to_vec(df_train, bow_train)\n",
        "vec_test = map_to_vec(df_test, bow_train)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x3j94wG9IV8T"
      },
      "source": [
        "## 2. Train & Test classification models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VGPB48U6klwU"
      },
      "source": [
        "X_train = vec_train[:,:-1]\n",
        "y_train = vec_train[:,-1]\n",
        "\n",
        "X_test = vec_test[:,:-1]\n",
        "y_test = vec_test[:,-1]"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k3NQn-NLVgwR",
        "outputId": "3c3cbbe1-ece1-4b14-bd31-ebe7b5a155b4"
      },
      "source": [
        "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((17528, 435), (17528,), (4381, 435), (4381,))"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N1oM0vw0JetC"
      },
      "source": [
        "***Build a Classification Model 1***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G94VaWuI8axU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "outputId": "d036a8e4-53e4-41fe-9e4e-5b6999f062d0"
      },
      "source": [
        "training_time = []\n",
        "test_time = []\n",
        "test_acc = []\n",
        "\n",
        "# define the support vector machine model\n",
        "clf = svm.SVC(kernel = 'linear', random_state = 0, probability=True)\n",
        "\n",
        "# fit the model on the whole dataset\n",
        "t0 = time.time()\n",
        "clf.fit(X_train, y_train)\n",
        "t1 = time.time() - t0\n",
        "training_time.append(t1)\n",
        "\n",
        "# predict the class label\n",
        "t0 = time.time()\n",
        "pred_clf = clf.predict(X_test)\n",
        "t1 = time.time() - t0\n",
        "test_time.append(t1)\n",
        "\n",
        "# classification accuracy\n",
        "test_acc.append(accuracy_score(y_test, pred_clf))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-d893a79a45b4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m# classification accuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0macc_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_clf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'acc_test' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bEch93Lq7WjI"
      },
      "source": [
        "# classification accuracy\n",
        "test_acc.append(accuracy_score(y_test, pred_clf))"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lqct2Tp1UCQ3"
      },
      "source": [
        "***Build a Classification Model 2***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CbD3edQK7btz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2af65af0-e257-4a65-b78b-f260e50734cf"
      },
      "source": [
        "# define the multinomial logistic regression model\n",
        "lrc = LogisticRegression(multi_class='multinomial', solver='lbfgs')\n",
        "\n",
        "# fit the model on the whole dataset\n",
        "t0 = time.time()\n",
        "lrc.fit(X_train, y_train)\n",
        "t1 = time.time() - t0\n",
        "training_time.append(t1)\n",
        "\n",
        "# predict the class label\n",
        "t0 = time.time()\n",
        "pred_lrc = lrc.predict(X_test)\n",
        "t1 = time.time() - t0\n",
        "test_time.append(t1)\n",
        "\n",
        "# classification accuracy\n",
        "test_acc.append(accuracy_score(y_test, pred_lrc))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Khk3TzEpSYqi"
      },
      "source": [
        "***Build a Classification Model 3***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "maBEYh-uzdyH"
      },
      "source": [
        "# define the multi layer perceptrons model\n",
        "mlpc = MLPClassifier(hidden_layer_sizes = (11, 11, 11), max_iter = 500)\n",
        "\n",
        "# fit the model on the whole dataset\n",
        "t0 = time.time()\n",
        "mlpc.fit(X_train, y_train)\n",
        "t1 = time.time() - t0\n",
        "training_time.append(t1)\n",
        "\n",
        "# predict the class label\n",
        "t0 = time.time()\n",
        "pred_mlpc = mlpc.predict(X_test)\n",
        "t1 = time.time() - t0\n",
        "test_time.append(t1)\n",
        "\n",
        "# classification accuracy\n",
        "test_acc.append(accuracy_score(y_test, pred_mlpc))"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wjdOm8-L-IU-"
      },
      "source": [
        "## 3. Results & Evaluation "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "872YmRSHJomK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39fab9f4-21ec-4a35-ed9e-4071a6f07d82"
      },
      "source": [
        "# results\n",
        "print(classification_report(y_test, pred_mlpc))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         1.0       0.74      0.51      0.60       688\n",
            "         2.0       0.82      0.62      0.71      1248\n",
            "         3.0       0.75      0.91      0.82      2445\n",
            "\n",
            "    accuracy                           0.77      4381\n",
            "   macro avg       0.77      0.68      0.71      4381\n",
            "weighted avg       0.77      0.77      0.76      4381\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MjI7nZPdVWQy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62ff7537-43fd-45c3-947b-68ea2c5ed451"
      },
      "source": [
        "training_time, test_time, test_acc"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([475.8613905906677, 5.536408424377441, 75.04947209358215],\n",
              " [17.699882984161377, 0.008712291717529297, 0.01678323745727539],\n",
              " [0.7671764437343073, 0.7701438027847524, 0.7655786350148368])"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yC1_YaD9e0tS"
      },
      "source": [
        "# define the model evaluation procedure\n",
        "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
        "\n",
        "# evaluate the model and collect the scores\n",
        "n_scores1 = cross_val_score(clf, X_test, y_test, scoring='accuracy', cv=cv, n_jobs=-1)\n",
        "n_scores2 = cross_val_score(lrc, X_test, y_test, scoring='accuracy', cv=cv, n_jobs=-1)\n",
        "n_scores3 = cross_val_score(mlpc, X_test, y_test, scoring='accuracy', cv=cv, n_jobs=-1)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6_0agPccUsLx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad6a8727-bdfc-4865-9e14-c6c3e38ab580"
      },
      "source": [
        "# report the model performance\n",
        "print('Mean Accuracy: %.3f (%.3f)' % (mean(n_scores1), std(n_scores1)))\n",
        "print('Mean Accuracy: %.3f (%.3f)' % (mean(n_scores2), std(n_scores2)))\n",
        "print('Mean Accuracy: %.3f (%.3f)' % (mean(n_scores3), std(n_scores3)))"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Accuracy: 0.760 (0.015)\n",
            "Mean Accuracy: 0.766 (0.014)\n",
            "Mean Accuracy: 0.734 (0.018)\n"
          ]
        }
      ]
    }
  ]
}