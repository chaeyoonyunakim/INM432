{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Environment setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basics\n",
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "np.random.seed(20211001)\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\chaey\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\chaey\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\chaey\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nltk imports\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "#set(stopwords.words('english'))\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Algos\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scikit-learn Tools for modelling\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_recall_fscore_support, confusion_matrix, classification_report, accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading & Manipulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tabular representation of the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>question</th>\n",
       "      <th>category</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dbpedia_1177</td>\n",
       "      <td>Was Jacqueline Kennedy Onassis a follower of M...</td>\n",
       "      <td>boolean</td>\n",
       "      <td>[boolean]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dbpedia_14427</td>\n",
       "      <td>What is the name of the opera based on Twelfth...</td>\n",
       "      <td>resource</td>\n",
       "      <td>[dbo:Opera, dbo:MusicalWork, dbo:Work]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dbpedia_16615</td>\n",
       "      <td>When did Lena Horne receive the Grammy Award f...</td>\n",
       "      <td>literal</td>\n",
       "      <td>[date]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              id                                           question  category  \\\n",
       "0   dbpedia_1177  Was Jacqueline Kennedy Onassis a follower of M...   boolean   \n",
       "1  dbpedia_14427  What is the name of the opera based on Twelfth...  resource   \n",
       "2  dbpedia_16615  When did Lena Horne receive the Grammy Award f...   literal   \n",
       "\n",
       "                                     type  \n",
       "0                               [boolean]  \n",
       "1  [dbo:Opera, dbo:MusicalWork, dbo:Work]  \n",
       "2                                  [date]  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_json('C:/Users/chaey/INM713-python-main/smart-dataset-master/datasets/DBpedia/smarttask_dbpedia_train.json')\n",
    "df_test = pd.read_json('C:/Users/chaey/INM713-python-main/smart-dataset-master/datasets/DBpedia/smarttask_dbpedia_test.json')\n",
    "df_train.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((17571, 4), (4381, 4))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape, df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id           0\n",
       "question    43\n",
       "category     0\n",
       "type         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17528, 4)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.dropna(subset=['id', 'question', 'category'], inplace=True)\n",
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>question</th>\n",
       "      <th>category</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dbpedia_16015</td>\n",
       "      <td>How many ingredients are in the grain} ?</td>\n",
       "      <td>literal</td>\n",
       "      <td>[number]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dbpedia_3885</td>\n",
       "      <td>Is the case fatality rate of Fournier gangrene...</td>\n",
       "      <td>boolean</td>\n",
       "      <td>[boolean]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dbpedia_12907</td>\n",
       "      <td>Does the shelf life of spinach equal 8?</td>\n",
       "      <td>boolean</td>\n",
       "      <td>[boolean]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dbpedia_7955</td>\n",
       "      <td>What sound does a pig make in the French langu...</td>\n",
       "      <td>literal</td>\n",
       "      <td>[string]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dbpedia_2376</td>\n",
       "      <td>When was Fergie completed his record label in ...</td>\n",
       "      <td>literal</td>\n",
       "      <td>[date]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>dbpedia_4197</td>\n",
       "      <td>Which are the coordinates of easternmost point...</td>\n",
       "      <td>literal</td>\n",
       "      <td>[string]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>dbpedia_22599</td>\n",
       "      <td>Where did the war take place where one of the ...</td>\n",
       "      <td>resource</td>\n",
       "      <td>[dbo:Country, dbo:PopulatedPlace, dbo:Place]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>dbpedia_5469</td>\n",
       "      <td>Was Michael Bloomberg a residence in Medford a...</td>\n",
       "      <td>boolean</td>\n",
       "      <td>[boolean]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>dbpedia_687</td>\n",
       "      <td>How many platforms does Tomb Raider have?</td>\n",
       "      <td>literal</td>\n",
       "      <td>[number]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>dbpedia_19677</td>\n",
       "      <td>Which mountains are contained in Inyo National...</td>\n",
       "      <td>resource</td>\n",
       "      <td>[dbo:Mountain, dbo:NaturalPlace, dbo:Place]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              id                                           question  category  \\\n",
       "0  dbpedia_16015           How many ingredients are in the grain} ?   literal   \n",
       "1   dbpedia_3885  Is the case fatality rate of Fournier gangrene...   boolean   \n",
       "2  dbpedia_12907            Does the shelf life of spinach equal 8?   boolean   \n",
       "3   dbpedia_7955  What sound does a pig make in the French langu...   literal   \n",
       "4   dbpedia_2376  When was Fergie completed his record label in ...   literal   \n",
       "5   dbpedia_4197  Which are the coordinates of easternmost point...   literal   \n",
       "6  dbpedia_22599  Where did the war take place where one of the ...  resource   \n",
       "7   dbpedia_5469  Was Michael Bloomberg a residence in Medford a...   boolean   \n",
       "8    dbpedia_687          How many platforms does Tomb Raider have?   literal   \n",
       "9  dbpedia_19677  Which mountains are contained in Inyo National...  resource   \n",
       "\n",
       "                                           type  \n",
       "0                                      [number]  \n",
       "1                                     [boolean]  \n",
       "2                                     [boolean]  \n",
       "3                                      [string]  \n",
       "4                                        [date]  \n",
       "5                                      [string]  \n",
       "6  [dbo:Country, dbo:PopulatedPlace, dbo:Place]  \n",
       "7                                     [boolean]  \n",
       "8                                      [number]  \n",
       "9   [dbo:Mountain, dbo:NaturalPlace, dbo:Place]  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dbo:Country', 'dbo:PopulatedPlace', 'dbo:Place']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.iloc[6, 3][:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preprocessing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Question Parsing***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['How many ingredients are in the grain} ?',\n",
       "       'Is the case fatality rate of Fournier gangrene fewer than 9.0?',\n",
       "       'Does the shelf life of spinach equal 8?', ...,\n",
       "       'What is the location of Edmonton',\n",
       "       'In which department does Raymond Baldwin work?',\n",
       "       'What is Actorenregister ID for Utrecht University?'], dtype=object)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_train = df_train.question.values\n",
    "text_test = df_test.question.values\n",
    "text_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Clean the corpus***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training dataset\n",
    "vocab_train = []\n",
    "tokens_train = []\n",
    "\n",
    "for sent in text_train:\n",
    "    x = word_tokenize(sent) # tokenization (strip sentences by word)\n",
    "    sentence = [w.lower() for w in x if w.isalpha()] # lower alphabets (filtering non-string characters and then decapitalization)\n",
    " \n",
    "    for word in sentence:\n",
    "        if word not in vocab_train:\n",
    "            vocab_train.append(word) # remove duplicates\n",
    "            \n",
    "for word in vocab_train:\n",
    "    if word not in stopwords.words(): # filter stopwords out\n",
    "        tokens_train.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test dataset\n",
    "vocab_test = []\n",
    "tokens_test = []\n",
    "\n",
    "for sent in text_test:\n",
    "    x = word_tokenize(sent)\n",
    "    sentence = [w.lower() for w in x if w.isalpha()]\n",
    " \n",
    "    for word in sentence:\n",
    "        if word not in vocab_test:\n",
    "            vocab_test.append(word)\n",
    "            \n",
    "for word in vocab_test:\n",
    "    if word in tokens_train:\n",
    "        if word not in stopwords.words():\n",
    "            tokens_test.append(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Text normalization***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "ps = PorterStemmer()\n",
    "\n",
    "def normalize_word(w):\n",
    "    word1 = wordnet_lemmatizer.lemmatize(w, pos = \"n\")\n",
    "    word2 = wordnet_lemmatizer.lemmatize(word1, pos = \"v\")\n",
    "    word3 = wordnet_lemmatizer.lemmatize(word2, pos = (\"a\"))\n",
    "    word = ps.stem(word3)\n",
    "    return word\n",
    "\n",
    "\n",
    "## Define the Bag of Words model function\n",
    "def create_bow(word_list):\n",
    "    ind = 0 \n",
    "    bow = {}\n",
    "    for w in word_list:\n",
    "        _w = normalize_word(w)\n",
    "        if _w not in bow:\n",
    "            bow[_w] = ind \n",
    "            ind += 1 \n",
    "    return bow\n",
    "\n",
    "bow_train = create_bow(tokens_train)\n",
    "bow_test = create_bow(tokens_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Vectorization***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Assign an index to the word\n",
    "label_map = {\"boolean\": 1, \"literal\":2, \"resource\":3}\n",
    "\n",
    "def map_to_vec(df, bow):\n",
    "    # add 1 for now just for the category\n",
    "    # requires additional cols for literals and sub resources for later\n",
    "    ncols = len(bow) + 1 \n",
    "    data = np.zeros(shape = (df.shape[0], ncols))\n",
    "  \n",
    "    for i in range(df.shape[0]):\n",
    "        # set the label\n",
    "        data[i, -1] = label_map[df.iloc[i, 2]]\n",
    "        # parse the sentence\n",
    "        que = df.iloc[i, 1]\n",
    "        for w in word_tokenize(que):\n",
    "            w = w.lower()\n",
    "            if w.isalpha():\n",
    "                # normalize word\n",
    "                w_norm = normalize_word(w)\n",
    "                if w_norm in bow:\n",
    "                    # print(f\"({i}, {w_norm})\")\n",
    "                    data[i, bow[w_norm]] += 1 \n",
    "    return data\n",
    "\n",
    "vec_train = map_to_vec(df_train, bow_train)\n",
    "vec_test = map_to_vec(df_test, bow_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index dictionary to assign an index to each type in dataset\n",
    "\n",
    "def sub_to_vec(df, vec):\n",
    "    target = np.zeros(shape = (df.shape[0], 1))\n",
    "    \n",
    "    for i in range(df.shape[0]):\n",
    "        if vec[i,-1] <= 2:\n",
    "            # set the label\n",
    "            type_map = {\"boolean\": 1, \"date\":2, \"number\":3, \"string\":4}\n",
    "            target[i, -1] = type_map[df.iloc[i, 3][0]]\n",
    "    return target\n",
    "\n",
    "y_train = sub_to_vec(df_train, vec_train)\n",
    "y_test = sub_to_vec(df_test, vec_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Train & Test classification models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = vec_train[:,:-1]\n",
    "y_train = y_train\n",
    "\n",
    "X_test = vec_test[:,:-1]\n",
    "y_test = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Build a Classification Model 1***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "training_time = []\n",
    "test_time = []\n",
    "test_acc = []\n",
    "\n",
    "# define the support vector machine model\n",
    "clf = svm.SVC(kernel = 'linear', random_state = 0, probability=True)\n",
    "\n",
    "# fit the model on the whole dataset\n",
    "t0 = time.time()\n",
    "clf.fit(X_train, y_train)\n",
    "t1 = time.time() - t0\n",
    "training_time.append(t1)\n",
    "\n",
    "# predict the class label\n",
    "t0 = time.time()\n",
    "pred_clf = clf.predict(X_test)\n",
    "t1 = time.time() - t0\n",
    "test_time.append(t1)\n",
    "\n",
    "# classification accuracy\n",
    "test_acc.append(accuracy_score(y_test, pred_clf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Build a Classification Model 2***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "# define the multinomial logistic regression model\n",
    "lrc = LogisticRegression(multi_class='multinomial', solver='lbfgs')\n",
    "\n",
    "# fit the model on the whole dataset\n",
    "t0 = time.time()\n",
    "lrc.fit(X_train, y_train)\n",
    "t1 = time.time() - t0\n",
    "training_time.append(t1)\n",
    "\n",
    "# predict the class label\n",
    "t0 = time.time()\n",
    "pred_lrc = lrc.predict(X_test)\n",
    "t1 = time.time() - t0\n",
    "test_time.append(t1)\n",
    "\n",
    "# classification accuracy\n",
    "test_acc.append(accuracy_score(y_test, pred_lrc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Build a Classification Model 3***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "# define the multi layer perceptrons model\n",
    "mlpc = MLPClassifier(hidden_layer_sizes = (11, 11, 11), max_iter = 500)\n",
    "\n",
    "# fit the model on the whole dataset\n",
    "t0 = time.time()\n",
    "mlpc.fit(X_train, y_train)\n",
    "t1 = time.time() - t0\n",
    "training_time.append(t1)\n",
    "\n",
    "# predict the class label\n",
    "t0 = time.time()\n",
    "pred_mlpc = mlpc.predict(X_test)\n",
    "t1 = time.time() - t0\n",
    "test_time.append(t1)\n",
    "\n",
    "# classification accuracy\n",
    "test_acc.append(accuracy_score(y_test, pred_mlpc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Results & Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.89      0.94      0.91      2445\n",
      "         1.0       0.86      0.86      0.86       688\n",
      "         2.0       0.77      0.70      0.73       316\n",
      "         3.0       0.89      0.78      0.83       407\n",
      "         4.0       0.93      0.80      0.86       525\n",
      "\n",
      "    accuracy                           0.88      4381\n",
      "   macro avg       0.87      0.82      0.84      4381\n",
      "weighted avg       0.88      0.88      0.88      4381\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# results\n",
    "print(classification_report(y_test, pred_clf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.89      0.95      0.92      2445\n",
      "         1.0       0.85      0.89      0.87       688\n",
      "         2.0       0.81      0.72      0.76       316\n",
      "         3.0       0.92      0.75      0.83       407\n",
      "         4.0       0.96      0.78      0.86       525\n",
      "\n",
      "    accuracy                           0.89      4381\n",
      "   macro avg       0.89      0.82      0.85      4381\n",
      "weighted avg       0.89      0.89      0.89      4381\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, pred_lrc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.86      0.91      0.89      2445\n",
      "         1.0       0.83      0.75      0.79       688\n",
      "         2.0       0.64      0.59      0.61       316\n",
      "         3.0       0.76      0.74      0.75       407\n",
      "         4.0       0.84      0.78      0.81       525\n",
      "\n",
      "    accuracy                           0.83      4381\n",
      "   macro avg       0.79      0.76      0.77      4381\n",
      "weighted avg       0.83      0.83      0.83      4381\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, pred_mlpc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([14734.663571357727, 73.42798662185669, 178.94628405570984],\n",
       " [743.3649921417236, 1.3010668754577637, 0.7436397075653076],\n",
       " [0.8801643460397169, 0.8886099064140607, 0.8326866012325953])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_time, test_time, test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 498. MiB for an array with shape (3943, 16546) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31m_RemoteTraceback\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;31m_RemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 431, in _process_worker\n    r = call_item()\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 285, in __call__\n    return self.fn(*self.args, **self.kwargs)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 595, in __call__\n    return self.func(*args, **kwargs)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 262, in __call__\n    return [func(*args, **kwargs)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 262, in <listcomp>\n    return [func(*args, **kwargs)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 524, in _fit_and_score\n    X_train, y_train = _safe_split(estimator, X, y, train)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\metaestimators.py\", line 204, in _safe_split\n    X_subset = _safe_indexing(X, indices)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\__init__.py\", line 395, in _safe_indexing\n    return _array_indexing(X, indices, indices_dtype, axis=axis)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\__init__.py\", line 181, in _array_indexing\n    return array[key] if axis == 0 else array[:, key]\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\core\\memmap.py\", line 331, in __getitem__\n    res = super(memmap, self).__getitem__(index)\nMemoryError: Unable to allocate 498. MiB for an array with shape (3943, 16546) and data type float64\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-117-a5cd278f2155>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# evaluate the model and collect the scores\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mn_scores1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'accuracy'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mn_scores2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'accuracy'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mn_scores3\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmlpc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'accuracy'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     70\u001b[0m                           FutureWarning)\n\u001b[0;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36mcross_val_score\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, error_score)\u001b[0m\n\u001b[0;32m    399\u001b[0m     \u001b[0mscorer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_scoring\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mscoring\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    400\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 401\u001b[1;33m     cv_results = cross_validate(estimator=estimator, X=X, y=y, groups=groups,\n\u001b[0m\u001b[0;32m    402\u001b[0m                                 \u001b[0mscoring\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'score'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mscorer\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcv\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    403\u001b[0m                                 \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     70\u001b[0m                           FutureWarning)\n\u001b[0;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36mcross_validate\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, error_score)\u001b[0m\n\u001b[0;32m    240\u001b[0m     parallel = Parallel(n_jobs=n_jobs, verbose=verbose,\n\u001b[0;32m    241\u001b[0m                         pre_dispatch=pre_dispatch)\n\u001b[1;32m--> 242\u001b[1;33m     scores = parallel(\n\u001b[0m\u001b[0;32m    243\u001b[0m         delayed(_fit_and_score)(\n\u001b[0;32m    244\u001b[0m             \u001b[0mclone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscorers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1059\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1060\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1061\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1062\u001b[0m             \u001b[1;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1063\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    938\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    939\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'supports_timeout'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 940\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    941\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    942\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[1;34m(future, timeout)\u001b[0m\n\u001b[0;32m    540\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[0;32m    541\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 542\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    543\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mCfTimeoutError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\concurrent\\futures\\_base.py\u001b[0m in \u001b[0;36mresult\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    430\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mCancelledError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    431\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mFINISHED\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 432\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    433\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    434\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\concurrent\\futures\\_base.py\u001b[0m in \u001b[0;36m__get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    386\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__get_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    387\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 388\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    389\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    390\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 498. MiB for an array with shape (3943, 16546) and data type float64"
     ]
    }
   ],
   "source": [
    "# define the model evaluation procedure\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "\n",
    "# evaluate the model and collect the scores\n",
    "n_scores1 = cross_val_score(clf, X_test, y_test, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "n_scores2 = cross_val_score(lrc, X_test, y_test, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "n_scores3 = cross_val_score(mlpc, X_test, y_test, scoring='accuracy', cv=cv, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# report the model performance\n",
    "print('Mean Accuracy: %.3f (%.3f)' % (mean(n_scores1), std(n_scores1)))\n",
    "print('Mean Accuracy: %.3f (%.3f)' % (mean(n_scores2), std(n_scores2)))\n",
    "print('Mean Accuracy: %.3f (%.3f)' % (mean(n_scores3), std(n_scores3)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
